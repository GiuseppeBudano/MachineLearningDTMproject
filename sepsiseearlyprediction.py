# -*- coding: utf-8 -*-
"""SepsisEEarlyPrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qeQ2NQrOr939CVCtAiVBMOqr6vawhSaO

# MIMIC-IV Project: Sepsis Prediction & LOS Regression

**Goal**: The objective is twofold, addressing two different machine learning tasks:
1.  **Task A (Classification):** Predict the onset of sepsis (`is_sepsis`).
2.  **Task B (Regression):** Predict the Length of Stay in the Intensive Care Unit (`LOS_ICU`).

**Dataset**: Data extracted from the MIMIC-IV clinical database (Admissions, Diagnosis, Vitals, Labs, Medications).

**Requirements**:
1.  **Cohort Selection**: Focus on adult patients, first ICU admission only.
2.  **Feature Engineering**: Advanced time-window processing and dynamic statistics computation.
3.  **Modeling**: Comparison between manual `scikit-learn` models (Random Forest) and `AutoML` (FLAML).

# Setup and Libraries
Import necessary libraries and configure the environment settings.
Note: Since data from MIMIC-IV cannot be uploaded on GitHub, I am using Google drive to direct to it. It will be discussed at the exam
"""

# Install FLAML - Missing in the original file but required for the code to run
!pip install flaml

import pandas as pd
import numpy as np
import os
import gc
from google.colab import drive
import warnings

warnings.filterwarnings('ignore')

# 1. Drive Mounting
if not os.path.exists('/content/drive'):
    drive.mount('/content/drive')

BASE_PATH = '/content/drive/MyDrive/MIMIC_IV/'

# --- A. CLINICAL DEFINITIONS (ICD CODES) ---
SEPSIS_PREFIXES = ['99591', '99592', '78552', '038', '670', 'A40', 'A41', 'R652', 'O85']
NEWBORN_PREFIXES = ['P36', '77', 'V3', 'Z38']

COMORBIDITY_MAP = {
    'Diabetes': ['250', 'E10', 'E11'],
    'Hypertension': ['401', 'I10'],
    'CHF': ['428', 'I50'],
    'CKD': ['585', 'N18'],
    'Cancer': ['140', '199', 'C00', 'C97']
}

def load_smart(path):
    try:
        if not os.path.exists(path):
            print(f"‚ö†Ô∏è Missing file: {path}")
            return None
        df = pd.read_csv(path)
        if len(df.columns) == 1 and ';' in df.columns[0]:
            df = pd.read_csv(path, sep=';')
        df.columns = df.columns.str.strip().str.lower()
        return df
    except Exception as e:
        print(f"Error loading {path}: {e}")
        return None

"""# Data Loading & Cohort Definition

In this section, we load the raw MIMIC-IV CSV files (`diagnoses_icd`, `admissions`, `patients`, `icustays`).
We define the study cohort by:
1.  Excluding newborns.
2.  Selecting only the **first ICU stay** for each admission (Index Event).
3.  Filtering for adult patients (age >= 18).
"""

print("1. Loading and Defining Cohort...")

df_diag = load_smart(BASE_PATH + 'diagnoses_icd.csv')
df_adm = load_smart(BASE_PATH + 'admissions.csv')
df_pat = load_smart(BASE_PATH + 'patients.csv')
df_icu = load_smart(BASE_PATH + 'icustays.csv')

# --- C. FILTERING AND MASTER DATASET CREATION ---
newborn_mask = df_diag['icd_code'].astype(str).str.startswith(tuple(NEWBORN_PREFIXES))
exclude_ids = df_diag.loc[newborn_mask, 'subject_id'].unique()
df_adm = df_adm[~df_adm['subject_id'].isin(exclude_ids)]

print("   -> Selecting First ICU Stay (Index Event)...")
df_icu = df_icu.sort_values(['subject_id', 'hadm_id', 'intime'])
df_icu_first = df_icu.groupby('hadm_id').first().reset_index()

df_cohort = pd.merge(df_icu_first, df_adm[['subject_id', 'hadm_id', 'admittime', 'dischtime', 'hospital_expire_flag']], on=['subject_id', 'hadm_id'], how='left')
df_cohort = pd.merge(df_cohort, df_pat[['subject_id', 'gender', 'anchor_age', 'anchor_year']], on='subject_id', how='left')

df_cohort['intime'] = pd.to_datetime(df_cohort['intime'])
df_cohort['age'] = df_cohort['anchor_age'] + (df_cohort['intime'].dt.year - df_cohort['anchor_year'])
df_cohort = df_cohort[df_cohort['age'] >= 18]

"""# Feature Engineering

We perform feature extraction in four steps:
1.  **Comorbidities**: Extraction of risk factors (Diabetes, CHF, etc.) from ICD codes.
2.  **Targets & Demographics**: Definition of Sepsis (Binary) and LOS (Regression), plus Gender encoding.
3.  **Dynamic Features**: Aggregation of Vitals and Labs within the first 6 hours.
4.  **Treatments**: Identification of Vasopressor usage.
###2.1 Comorbidities and target
Extraction of risk factors (Diabetes, CHF, etc.) from ICD codes
"""

print("   -> Extracting Comorbidities (Charlson proxy)...")
df_diag['icd_str'] = df_diag['icd_code'].astype(str)
comorb_features = []
for name, prefixes in COMORBIDITY_MAP.items():
    mask = df_diag['icd_str'].str.startswith(tuple(prefixes))
    sick_adm_ids = df_diag.loc[mask, 'hadm_id'].unique()
    col_name = f'has_{name}'
    df_cohort[col_name] = df_cohort['hadm_id'].isin(sick_adm_ids).astype(int)
    comorb_features.append(col_name)

print("   -> Defining Sepsis Target...")
sepsis_mask = df_diag['icd_str'].str.startswith(tuple(SEPSIS_PREFIXES))
sepsis_adm_ids = df_diag.loc[sepsis_mask, 'hadm_id'].unique()
df_cohort['is_sepsis'] = df_cohort['hadm_id'].isin(sepsis_adm_ids).astype(int)

df_cohort.rename(columns={'los': 'LOS_ICU'}, inplace=True)
df_cohort = df_cohort[(df_cohort['LOS_ICU'] > 0) & (df_cohort['LOS_ICU'] < 100)]
df_cohort['gender_num'] = df_cohort['gender'].apply(lambda x: 1 if str(x).upper() == 'M' else 0)

cols_to_keep = ['subject_id', 'hadm_id', 'stay_id', 'intime', 'outtime', 'age', 'gender_num', 'is_sepsis', 'LOS_ICU'] + comorb_features
df_master = df_cohort[cols_to_keep].copy()

print(f"\n‚úÖ Initial Cohort Ready: {df_master.shape[0]} unique ICU stays.")

"""### 2.2 Dynamic Feature Extraction (Vitals & Labs)
Extracting statistical features (min, max, mean, trend) from the first 6 hours of ICU stay.
"""

VITAL_ITEM_IDS = {220045: 'HeartRate', 220179: 'SysBP', 220180: 'DiasBP', 220210: 'RespRate', 223761: 'TempF', 220277: 'SpO2'}
LAB_ITEM_IDS = {51301: 'WBC', 50813: 'Lactate', 50912: 'Creatinine', 51265: 'Platelet'}

def extract_dynamic_stats(filename, target_ids_map, time_window=6):
    file_path = '/content/' + filename if os.path.exists('/content/' + filename) else BASE_PATH + filename
    if not os.path.exists(file_path): return None
    print(f"   -> Processing {filename} (Window: 0-{time_window}h)...")
    valid_ids = list(target_ids_map.keys())
    ref_dates = df_master[['subject_id', 'hadm_id', 'stay_id', 'intime']].drop_duplicates()
    chunks = []
    cols_to_read = ['subject_id', 'hadm_id', 'itemid', 'charttime', 'valuenum']
    try:
        reader = pd.read_csv(file_path, chunksize=1000000, usecols=cols_to_read)
    except ValueError:
        cols_to_read = ['hadm_id', 'itemid', 'charttime', 'valuenum']
        reader = pd.read_csv(file_path, chunksize=1000000, usecols=cols_to_read)

    for i, chunk in enumerate(reader):
        chunk = chunk[chunk['itemid'].isin(valid_ids)]
        chunk['valuenum'] = pd.to_numeric(chunk['valuenum'], errors='coerce')
        chunk = chunk.dropna(subset=['valuenum'])
        if chunk.empty: continue
        chunk = pd.merge(chunk, ref_dates, on='hadm_id', how='inner')
        chunk['charttime'] = pd.to_datetime(chunk['charttime'])
        chunk['hours_from_admit'] = (chunk['charttime'] - chunk['intime']).dt.total_seconds() / 3600
        chunk = chunk[(chunk['hours_from_admit'] >= -2) & (chunk['hours_from_admit'] <= time_window)]
        if not chunk.empty: chunks.append(chunk[['stay_id', 'itemid', 'valuenum', 'charttime']])
        if (i+1) % 50 == 0: gc.collect()

    if not chunks: return None
    df_concat = pd.concat(chunks).sort_values(['stay_id', 'charttime'])
    df_pivot = df_concat.groupby(['stay_id', 'itemid'])['valuenum'].agg(['min', 'max', 'median', 'std', 'first', 'last']).unstack()
    df_pivot.columns = [f"{target_ids_map[item_id]}_{agg}" for agg, item_id in df_pivot.columns]
    df_result = df_pivot.reset_index()

    for item in set(target_ids_map.values()):
        if f"{item}_last" in df_result.columns and f"{item}_first" in df_result.columns:
            df_result[f"{item}_delta"] = df_result[f"{item}_last"] - df_result[f"{item}_first"]
    return df_result

df_vitals = extract_dynamic_stats('chartevents_vitals_small.csv', VITAL_ITEM_IDS)
df_labs = extract_dynamic_stats('labevents.csv', LAB_ITEM_IDS)

df_final = df_master.copy()
if df_labs is not None: df_final = pd.merge(df_final, df_labs, on='stay_id', how='left')
if df_vitals is not None: df_final = pd.merge(df_final, df_vitals, on='stay_id', how='left')

# Vasopressors
filename = 'vasopressors_only.csv'
file_path = BASE_PATH + filename
if os.path.exists(file_path):
    df_drugs = pd.read_csv(file_path)
    df_drugs.columns = [c.lower() for c in df_drugs.columns]
    ref_time = df_master[['stay_id', 'intime']].drop_duplicates()
    df_drugs = pd.merge(df_drugs, ref_time, on='stay_id', how='inner')
    df_drugs['hours'] = (pd.to_datetime(df_drugs['starttime']) - pd.to_datetime(df_drugs['intime'])).dt.total_seconds() / 3600
    vaso_stays = df_drugs[(df_drugs['hours'] >= -2) & (df_drugs['hours'] <= 6)]['stay_id'].unique()
    df_final['on_vasopressor'] = df_final['stay_id'].isin(vaso_stays).astype(int)
else:
    df_final['on_vasopressor'] = 0

#Blocco 5: Preprocessing & Split
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler

print("5. Preprocessing Finale (Gestione Missing & Outlier)...")

# 1. Pulizia: Teniamo solo numeri ed eliminiamo ID e target dalle feature
# Nota: gender_num c'√® gi√†, eliminiamo le stringhe e le date
cols_to_drop = ['subject_id', 'hadm_id', 'stay_id', 'intime', 'outtime',
                'icd_code', 'icd_str', 'anchor_year', 'dob', 'gender']
targets = ['is_sepsis', 'LOS_ICU']

# Selezioniamo solo colonne numeriche per X
X = df_final.select_dtypes(include=[np.number]).drop(columns=[c for c in cols_to_drop + targets if c in df_final.columns], errors='ignore')
y_sepsis = df_final['is_sepsis']

# 2. Split Stratificato (Per mantenere la % di sepsi uguale in train e test)
X_train_raw, X_test_raw, y_train, y_test = train_test_split(
    X, y_sepsis, test_size=0.3, stratify=y_sepsis, random_state=42
)

# 3. Gestione Missing Values (Qui rispondiamo alla tua domanda!)
# Usiamo la Mediana perch√© √® pi√π robusta agli outlier della Media
print("   -> Imputazione Missing Values (Mediana)...")
imputer = SimpleImputer(strategy='median')
X_train_imp = imputer.fit_transform(X_train_raw) # Impara dal train
X_test_imp = imputer.transform(X_test_raw)       # Applica al test

# 4. Scaling (StandardScaler)
# Importante per gestire gli Outlier (li riporta in scala gestibile)
print("   -> Standardizzazione...")
scaler = StandardScaler()
X_train = pd.DataFrame(scaler.fit_transform(X_train_imp), columns=X.columns, index=X_train_raw.index)
X_test = pd.DataFrame(scaler.transform(X_test_imp), columns=X.columns, index=X_test_raw.index)

# 5. Salviamo anche i target per la Regressione (Task B - Degenza)
# Cos√¨ siamo pronti per dopo senza rifare lo split
y_reg_train = df_final.loc[X_train.index, 'LOS_ICU']
y_reg_test = df_final.loc[X_test.index, 'LOS_ICU']

print(f"‚úÖ Dati Pronti! Train shape: {X_train.shape}")
print(f"   -> Missing Values rimasti: {X_train.isna().sum().sum()}")

"""# Data Understanding

### Dataframe
"""

# --- Blocco: Visualizzazione e Check Strutturale ---

print("--- Final Dataframe Info ---")
# Visualizza colonne, tipi di dati e valori non nulli
df_final.info()

# 4. Statistiche Descrittive Rapide (Tabella)
# Questo blocco mostra le medie delle variabili vitali principali separate per il target 'is_sepsis'
print("\n--- Statistiche Descrittive per Gruppo (Media) ---")

# Definiamo le variabili vitali principali per la tabella (come nel tuo codice originale)
key_vars = ['HeartRate_median', 'SysBP_median', 'RespRate_median', 'TempF_median']
present_vars = [v for v in key_vars if v in df_final.columns]

if present_vars:
    display(df_final.groupby('is_sepsis')[present_vars].mean())
else:
    print("‚ö†Ô∏è Attenzione: Le variabili vitali mediane non sono state trovate nel dataset per la visualizzazione.")

# Visualizzazione delle prime righe del dataset finale
print("\n--- Anteprima delle prime 5 righe (df_final) ---")
display(df_final.head())

"""### Visualization"""

# --- Block 4: Exploratory Data Analysis (EDA) & Visualization ---


import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Impostazioni stile (simile a pubblicazione)
sns.set_theme(style="whitegrid")
plt.rcParams['figure.figsize'] = (12, 6)

print("4. Analisi Esplorativa dei Dati (EDA)...")

# 1. Target balancing
# Fundamental (esp in medicine) if data are unbalanced
plt.figure(figsize=(8, 5))
ax = sns.countplot(x='is_sepsis', data=df_final, palette='viridis')
plt.title('Distribuzione del Target: Sepsis vs Non-Sepsis', fontsize=14)
plt.xlabel('Stato Sepsi (0=No, 1=S√¨)')
plt.ylabel('Numero di Pazienti')

# Aggiungiamo le percentuali sopra le barre
total = len(df_final)
for p in ax.patches:
    percentage = '{:.1f}%'.format(100 * p.get_height() / total)
    x = p.get_x() + p.get_width() / 2 - 0.05
    y = p.get_height()
    ax.annotate(percentage, (x, y), ha='center', va='bottom')
plt.show()

# 2. Boxplot Vitals at the admission
# Se le scatole sono diverse, la tua strategia di Risk Stratification √® valida.

# Select some key feature (if they exist in the dataset)
key_vars = ['HeartRate_median', 'SysBP_median', 'RespRate_median', 'TempF_median']
present_vars = [v for v in key_vars if v in df_final.columns]

if present_vars:
    # Creiamo una figura con sottografici
    fig, axes = plt.subplots(1, len(present_vars), figsize=(18, 6))

    for i, var in enumerate(present_vars):
        # Boxplot separando per Sepsi
        sns.boxplot(x='is_sepsis', y=var, data=df_final, ax=axes[i], palette="Set2", showfliers=False) # showfliers=False nasconde gli outlier estremi per pulizia
        axes[i].set_title(f'Distribuzione: {var.split("_")[0]}', fontsize=12)
        axes[i].set_xlabel('Sepsi')
        axes[i].set_ylabel('')

    plt.suptitle('Confronto Parametri Vitali (0-6h): Sepsis vs Controllo', fontsize=16)
    plt.tight_layout()
    plt.show()
else:
    print("‚ö†Ô∏è Attenzione: Le variabili vitali mediane non sono state trovate nel dataset.")

# 3. Correlazioni (Stile Prof: Heatmap)
# Vediamo se le variabili sono correlate tra loro (es. Sistolica vs Diastolica)
print("   -> Analisi delle Correlazioni (Top 10 Feature)...")

# Calcoliamo la correlazione solo sulle colonne numeriche
# Per leggibilit√†, prendiamo solo le feature vitali principali + target
cols_corr = present_vars + ['is_sepsis', 'age'] if present_vars else ['is_sepsis', 'age']
if 'Lactate_max' in df_final.columns: cols_corr.append('Lactate_max') # Se c'√® il lattato √® importante

corr_matrix = df_final[cols_corr].corr()

plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Matrice di Correlazione (Variabili Chiave)', fontsize=14)
plt.show()

# 4. Statistiche Descrittive Rapide (Tabella)
print("\n--- Statistiche Descrittive per Gruppo (Media) ---")
display(df_final.groupby('is_sepsis')[present_vars].mean())

"""# Modeling
 This section covers the training and evaluation of machine learning models.

 1.  **Data Preprocessing**: Splitting into Train/Test, handling missing values (Imputation), and Scaling.
 2.  **Task A: Classification (Sepsis)**:
     * **AutoML**: Using FLAML to find the best model.
     * **Threshold Tuning**: Optimizing the decision threshold to maximize F2-Score.
3.  **Task B: Regression (Length of Stay)**:
     * **Manual**: Random Forest Regressor baseline.
     * **AutoML**: FLAML optimization for regression.
"""

#Hyperparameter Tuning & Valutazione Finale ---
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import classification_report, confusion_matrix, fbeta_score, make_scorer
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

print("6. Ottimizzazione degli Iperparametri (Tuning)...")

# 1. Definiamo la "Griglia" dei parametri da testare
param_dist = {
    'n_estimators': [100, 200, 300],        # Numero di alberi
    'max_depth': [10, 20, 30, None],        # Profondit√† massima
    'min_samples_split': [2, 5, 10],        # Dati minimi per dividere un nodo
    'min_samples_leaf': [1, 2, 4],          # Dati minimi in una foglia
    'class_weight': ['balanced', 'balanced_subsample'] # Gestione sbilanciamento sepsi
}

# 2. Creiamo lo "Scorer" personalizzato (F2-Score)
# Beta=2 d√† pi√π importanza alla Recall (non perdere i malati)
ftwo_scorer = make_scorer(fbeta_score, beta=2)

# 3. Configurazione della Ricerca Casuale (RandomizedSearchCV)
rf_base = RandomForestClassifier(random_state=42, n_jobs=-1)

rf_random = RandomizedSearchCV(
    estimator=rf_base,
    param_distributions=param_dist,
    n_iter=20,              # Prova 20 combinazioni diverse
    scoring=ftwo_scorer,    # Ottimizza per F2-Score
    cv=3,                   # Cross-Validation a 3 pieghe
    verbose=1,
    random_state=42,
    n_jobs=-1               # Usa tutta la potenza della CPU
)

# 4. Avvio della Ricerca (Fit)
print("   -> Avvio ricerca della configurazione migliore (attendere prego)...")
rf_random.fit(X_train, y_train)

# 5. Risultati dell'Ottimizzazione
best_model = rf_random.best_estimator_
print(f"\n‚úÖ Trovata configurazione migliore:\n{rf_random.best_params_}")
print(f"   -> Miglior F2-Score in validazione: {rf_random.best_score_:.4f}")

# 6. Valutazione Finale sul Test Set
print("\n--- PERFORMANCE FINALI (Best Model su Test Set) ---")
y_pred = best_model.predict(X_test)

# Calcoli metriche
cm = confusion_matrix(y_test, y_pred)
f2 = fbeta_score(y_test, y_pred, beta=2)
tn, fp, fn, tp = cm.ravel()

print(f"Veri Positivi (Sepsi prese): {tp}")
print(f"Falsi Negativi (Sepsi perse): {fn}")
print(f"Recall (Sensibilit√†): {tp/(tp+fn):.2%}")
print(f"‚≠êÔ∏è F2-Score FINALE: {f2:.4f}")

# Plot Matrice di Confusione
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', cbar=False)
plt.title(f'Matrice di Confusione Ottimizzata (F2={f2:.2f})')
plt.ylabel('Reale')
plt.xlabel('Predetto')
plt.show()

# Feature Importance del modello vincitore
feat_importances = pd.Series(best_model.feature_importances_, index=X.columns)
plt.figure(figsize=(10, 6))
feat_importances.nlargest(10).plot(kind='barh', color='green')
plt.title('Top 10 Feature (Modello Ottimizzato)')
plt.xlabel('Importanza Relativa')
plt.show()

# --- Blocco 7: AutoML Benchmark (FLAML) ---
try:
    from flaml import AutoML
except ImportError:
    print("Installazione libreria FLAML...")
    !pip install flaml
    from flaml import AutoML

from sklearn.metrics import fbeta_score, roc_auc_score
import pandas as pd
import numpy as np

print("7. Esecuzione AutoML Benchmark...")

# Setup AutoML
automl_clf = AutoML()
settings_clf = {
    "time_budget": 180,
    "metric": 'roc_auc',
    "task": 'classification',
    "seed": 42,
    "verbose": 0,
    "eval_method": "cv",
    "n_splits": 3
}

# Addestramento
print("   -> Ricerca del miglior classificatore...")
automl_clf.fit(X_train, y_train, **settings_clf)

print(f"   üèÜ Best Algo: {automl_clf.best_estimator}")
print(f"   üèÜ Best Hyperparams: {automl_clf.best_config}")

# Predizioni Probabilit√†
y_probs_auto = automl_clf.predict_proba(X_test)[:, 1]
auc_auto = roc_auc_score(y_test, y_probs_auto)

# --- Blocco 9: Task B (Degenza) - MODELLO MANUALE ---
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error

print("9. Task B: Addestramento Modello Manuale (Random Forest)...")

# Configurazione Manuale: 100 alberi, profondit√† bilanciata
rf_reg_manual = RandomForestRegressor(n_estimators=100, max_depth=15, n_jobs=-1, random_state=42)

# Addestramento
rf_reg_manual.fit(X_train, y_reg_train)

# Predizione e Valutazione
y_pred_manual_reg = rf_reg_manual.predict(X_test)
mae_manual = mean_absolute_error(y_reg_test, y_pred_manual_reg)

print(f"‚úÖ MAE Modello Manuale: {mae_manual:.2f} giorni di errore medio.")

# --- Blocco 10: Task B (Degenza) - APPROCCIO AutoML ---
from flaml import AutoML
from sklearn.metrics import mean_absolute_error

print("10. Task B: Avvio AutoML per ottimizzazione Degenza...")

automl_reg = AutoML()
settings_reg = {
    "time_budget": 120,    # 2 minuti di ricerca
    "metric": 'mae',       # Ottimizziamo per minimizzare l'errore in giorni
    "task": 'regression',
    "seed": 42,
    "verbose": 0
}

# Ricerca automatica del miglior modello
automl_reg.fit(X_train, y_reg_train, **settings_reg)

# Miglior predizione
y_pred_auto_reg = automl_reg.predict(X_test)
mae_auto = mean_absolute_error(y_reg_test, y_pred_auto_reg)

print(f"üèÜ Miglior algoritmo trovato: {automl_reg.best_estimator}")
print(f"‚úÖ MAE Modello AutoML: {mae_auto:.2f} giorni di errore medio.")